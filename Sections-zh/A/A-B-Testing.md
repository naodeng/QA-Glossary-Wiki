# A/B 测试

<!-- TOC START -->
- [另请参阅：](#另请参阅：)
- [有关 A/B 测试的问题吗？](#有关-ab-测试的问题吗？)
  - [基础知识和重要性](#基础知识和重要性)
    - [什么是 A/B 测试？](#什么是-ab-测试？)
    - [为什么 A/B 测试很重要？](#为什么-ab-测试很重要？)
    - [A/B 测试的关键组成部分是什么？](#ab-测试的关键组成部分是什么？)
    - [A/B 测试与用户体验有何关系？](#ab-测试与用户体验有何关系？)
    - [A/B 测试在产品开发中的作用是什么？](#ab-测试在产品开发中的作用是什么？)
    - [A/B 测试如何设置？](#ab-测试如何设置？)
    - [进行 A/B 测试涉及哪些步骤？](#进行-ab-测试涉及哪些步骤？)
    - [A/B 测试常用的工具有哪些？](#ab-测试常用的工具有哪些？)
    - [如何确定 A/B 测试的样本量？](#如何确定-ab-测试的样本量？)
    - [A/B 测试中的对照和变体是什么？](#ab-测试中的对照和变体是什么？)
  - [分析与解读](#分析与解读)
    - [A/B 测试的结果如何分析？](#ab-测试的结果如何分析？)
    - [A/B 测试中使用哪些统计方法？](#ab-测试中使用哪些统计方法？)
    - [您如何解释 A/B 测试的结果？](#您如何解释-ab-测试的结果？)
    - [A/B 测试中的统计显着性是什么？](#ab-测试中的统计显着性是什么？)
    - [如何处理 A/B 测试中的误报或误报？](#如何处理-ab-测试中的误报或误报？)
  - [高级概念](#高级概念)
    - [什么是多变量测试以及它与 A/B 测试有何不同？](#什么是多变量测试以及它与-ab-测试有何不同？)
    - [什么是分割 URL 测试？](#什么是分割-url-测试？)
    - [A/B 测试有哪些限制？](#ab-测试有哪些限制？)
    - [A/B 测试如何与其他测试方法结合使用？](#ab-测试如何与其他测试方法结合使用？)
    - [A/B 测试中“均值回归”的概念是什么？](#ab-测试中均值回归的概念是什么？)
<!-- TOC END -->

（又名拆分测试）

A/B 测试

涉及创建网页的一个或多个变体以与当前版本进行比较。目标是根据特定指标（例如每位访问者的收入或转化率）确定哪个版本表现最佳。

## 另请参阅：

- [Wikipedia](https://en.wikipedia.org/wiki/A/B_testing)

## 有关 A/B 测试的问题吗？

### 基础知识和重要性

#### 什么是 A/B 测试？

[A/B testing](../A/a-b-testing.md)，也称为拆分测试，是一种将网页或应用程序的两个版本相互比较以确定哪个版本性能更好的方法。它涉及向用户随机显示两个变体（A 和 B），并使用统计分析来确定哪个版本在实现预定义目标方面更有效，例如提高点击率、转化率或任何其他关键[performance indicator](../P/performance-indicator.md)。
  在软件 [test automation](../T/test-automation.md) 的上下文中，[A/B testing](../A/a-b-testing.md) 可以自动运行对功能或界面的不同变体的测试，而无需手动干预。自动化 A/B 测试可以集成到持续集成/持续部署 (CI/CD) 管道中，以确保评估对应用程序所做的任何更改对用户行为和转化率的影响。
  为了自动化 A/B 测试，工程师通常结合使用功能标记和 [test automation](../T/test-automation.md) 框架。功能标志允许在功能的不同版本之间切换，而 [test automation](../T/test-automation.md) 框架执行测试并收集有关用户交互的数据。

  ```
  // Example of feature flagging in code
  if (featureFlagService.isFeatureEnabled('new-checkout-flow')) {
    // Variant B code
  } else {
    // Variant A code (control)
  }
  ```自动化[A/B testing](../A/a-b-testing.md) 可在软件开发中实现快速[iteration](../I/iteration.md) 和数据驱动的决策。通过利用自动化，团队可以扩展测试工作、减少人为错误并加速反馈循环，最终打造出更加以用户为中心的成功产品。

#### 为什么 A/B 测试很重要？

[A/B testing](../A/a-b-testing.md) 至关重要，因为它提供了关于变化对用户行为和转化率影响的**经验证据**。通过将控制版本 (A) 与变体 (B) 进行比较，它可以实现**数据驱动的决策**，从而实现**优化的性能**和**增强的用户满意度**。这种测试方法对于**验证有关用户偏好的假设**以及**识别软件应用程序的最有效元素**（例如按钮、图像或工作流程）特别有价值。
  在软件 [test automation](../T/test-automation.md) 的背景下，[A/B testing](../A/a-b-testing.md) 对于**迭代开发**很重要，使团队能够根据用户反馈**逐步改进**功能。它还可以在全面发布之前在较小的受众群体中进行测试，从而有助于**降低与新功能推出相关的风险**。此外，[A/B testing](../A/a-b-testing.md) 通过确保仅实施最具影响力的更改，有助于**最大化投资回报率**，从而**节省资源**并**将精力集中在对最终用户真正重要的事情上。
  对于[test automation](../T/test-automation.md) 工程师来说，将[A/B testing](../A/a-b-testing.md) 集成到自动化策略中可以带来更**稳健且以用户为中心的[test cases](../T/test-case.md)**，确保自动化测试不仅检查功能，而且还检查**真实世界的用户参与度和转化**。

#### A/B 测试的关键组成部分是什么？

A/B 测试的关键组成部分包括：

- **假设**：预测测试结果的明确陈述。
  - **变量**：变量中更改的元素，例如按钮颜色、文本或布局。
  - **测试组**：接收变体 (B) 的受众。
  - **对照组**：接收原始版本 (A) 的受众。
  - **随机化**：确保参与者被随机分配到测试组和对照组以消除偏见。
  - **成功指标**：用于确定测试结果的具体、可衡量的标准，例如转化率或点击率。
  - **持续时间**：运行测试的时间段，确保足够长的时间来收集重要数据。
  - **数据收集**：跟踪用户交互并根据成功指标衡量性能的机制。
  - **分析**：评估数据并确定性能差异是否显着的统计方法。
  - **细分**：按用户人口统计或行为细分数据，以了解对子群体的不同影响。
  在实践中，这些组件被集成到结构化流程中，以评估变更的影响并做出数据驱动的决策。 [Test automation](../T/test-automation.md)工程师应重点确保[test environment](../T/test-environment.md)稳定、数据收集准确、分析工具配置正确以有效解释结果。

- **假设**：预测测试结果的明确陈述。
  - **变量**：变量中更改的元素，例如按钮颜色、文本或布局。
  - **测试组**：接收变体 (B) 的受众。
  - **对照组**：接收原始版本 (A) 的受众。
  - **随机化**：确保参与者被随机分配到测试组和对照组以消除偏见。
  - **成功指标**：用于确定测试结果的具体、可衡量的标准，例如转化率或点击率。
  - **持续时间**：运行测试的时间段，确保足够长的时间来收集重要数据。
  - **数据收集**：跟踪用户交互并根据成功指标衡量性能的机制。
  - **分析**：评估数据并确定性能差异是否显着的统计方法。
  - **细分**：按用户人口统计或行为细分数据，以了解对子群体的不同影响。

#### A/B 测试与用户体验有何关系？

[A/B testing](../A/a-b-testing.md) 允许团队就软件产品的更改做出数据驱动的决策，从而直接影响**用户体验 (UX)**。通过比较功能或界面的两个版本（A 和 B），团队可以衡量每个变体在用户参与度、满意度和转化率方面的表现。然后可以为所有用户实施提供更好用户体验的变体，以增加页面停留时间、更高的点击率或提高所需操作的完成度等指标来表示。
  此过程确保更改不是基于假设或个人偏好，而是基于实际的用户行为。它有助于完善用户界面、工作流程和内容，以增强可用性和可访问性。 [A/B testing](../A/a-b-testing.md) 还可以在全面推出之前识别潜在的用户体验问题，从而降低负面用户反馈的风险以及昂贵的发布后修复的需要。
  通过根据 A/B 测试结果不断迭代和改进产品，公司可以提高用户满意度和忠诚度，这对于长期成功至关重要。本质上，[A/B testing](../A/a-b-testing.md) 充当用户反馈和产品演进之间的桥梁，促进以用户为中心的开发方法。

#### A/B 测试在产品开发中的作用是什么？

[A/B testing](../A/a-b-testing.md) 通过使团队能够做出**数据驱动的决策**，在产品开发中发挥着**关键作用**。它通过比较产品的两个版本来确定哪个版本在特定指标（例如转化率或用户参与度）方面表现更好，从而有助于优化特性和功能。
  在产品开发的背景下，[A/B testing](../A/a-b-testing.md) 用于**验证产品决策**并**降低与新功能发布相关的风险**。通过针对当前版本（控件）测试新功能（变体），开发人员和产品经理可以在将更改推广到整个用户群之前评估更改的影响。
  这种测试方法还支持**迭代开发**，允许根据用户反馈和行为不断改进产品。它可以通过提供用户喜欢或拒绝什么的证据来影响产品路线图，从而指导未来的开发优先事项。
  此外，[A/B testing](../A/a-b-testing.md) 可以集成到**敏捷工作流程**中，其中开发周期短和发布频繁是常见的。它允许快速实验和适应，这在快节奏的开发环境中至关重要。
  对于[test automation](../T/test-automation.md) 工程师来说，[A/B testing](../A/a-b-testing.md) 需要设置用户交互的**自动跟踪**和**分析**，以衡量不同变体的性能。工程师必须确保[test environment](../T/test-environment.md)稳定并且收集的数据对于准确决策来说是可靠的。
  总之，[A/B testing](../A/a-b-testing.md) 是产品开发中的**战略工具**，它可以增强用户体验、验证产品决策并培育持续改进的实验文化。

＃＃＃ 执行

#### A/B 测试如何设置？

设置 A/B 测试涉及以下步骤：

1. **定义目标**：明确说明您想要提高的目标（例如转化率、点击率）。
  2. **假设**：根据数据，对哪些变化可以带来改进做出有根据的猜测。
  3. **创建变体**：在一个或多个变体中实施更改，同时保留原始版本作为对照。
  4. **细分受众**：决定如何划分用户，确保他们被随机分配到对照组或变体组。
  5. **决定指标**：选择将衡量变体影响的关键 [performance indicators](../P/performance-indicator.md) (KPI)。
  6. **确保正确的跟踪**：设置跟踪工具来收集有关控件和变体的用户行为的数据。
  7. **运行测试**：启动实验，为用户提供足够的时间与两个版本进行交互。
  8. **监控测试**：检查是否存在任何技术问题并确保准确收集数据。
  9. **分析结果**：测试结束后，使用统计方法将变体的性能与对照进行比较。
  10. **做出决策**：根据分析，决定是否实施更改、运行额外测试或放弃变体。
  下面是一个简单的代码片段，用于说明如何将用户分配到 Web 应用程序中的不同组：

  ```
  function assignGroup(user) {
    const randomNumber = Math.random();
    return randomNumber < 0.5 ? 'control' : 'variant';
  }
  ```此函数使用随机数将用户按 50/50 分配到“控制”组或“变体”组。根据需要调整阈值以更改组之间的用户分布。

1. **定义目标**：明确说明您想要提高的目标（例如转化率、点击率）。
  2. **假设**：根据数据，对哪些变化可以带来改进做出有根据的猜测。
  3. **创建变体**：在一个或多个变体中实施更改，同时保留原始版本作为对照。
  4. **细分受众**：决定如何划分用户，确保他们被随机分配到对照组或变体组。
  5. **决定指标**：选择将衡量变体影响的关键 [performance indicators](../P/performance-indicator.md) (KPI)。
  6. **确保正确的跟踪**：设置跟踪工具来收集有关控件和变体的用户行为的数据。
  7. **运行测试**：启动实验，为用户提供足够的时间与两个版本进行交互。
  8. **监控测试**：检查是否存在任何技术问题并确保准确收集数据。
  9. **分析结果**：测试结束后，使用统计方法将变体的性能与对照进行比较。
  10. **做出决策**：根据分析，决定是否实施更改、运行额外测试或放弃变体。

#### 进行 A/B 测试涉及哪些步骤？

进行 A/B 测试涉及几个步骤：

1. **定义目标**：明确说明您希望通过测试实现什么目标，例如提高点击率或提高转化率。
  2. **制定假设**：根据您的目标，创建一个预测测试结果的假设。
  3. **识别变量**：确定与对照相比，您将在变体中更改的元素。
  4. **创建变体**：开发产品的替代版本，其中包括您想要测试的更改。
  5. **选择受众**：选择测试的目标受众，确保其代表您的用户群。
  6. **确定分配**：决定如何在对照组和变体组之间分配受众。
  7. **确保有效性**：检查您的测试是否存在可能影响结果的偏差和混杂变量。
  8. **运行测试**：向选定的受众部署 A/B 测试，监控每个组的表现。
  9. **收集数据**：收集有关每个组如何与相应版本的产品交互的数据。
  10. **分析结果**：使用统计方法确定对照和变体之间是否存在显着差异。
  11. **做出决定**：根据分析，决定是否实施更改、运行额外测试或放弃变体。
  12. **记录结果**：记录测试的结果和见解，以供将来参考和组织学习。
  13. **实施更改**：如果变体成功，请将更改推广到所有用户。
  请记住运行测试足够长的时间以收集足够的数据并避免根据不完整的结果做出决策。

1. **定义目标**：明确说明您希望通过测试实现什么目标，例如提高点击率或提高转化率。
  2. **制定假设**：根据您的目标，创建一个预测测试结果的假设。
  3. **识别变量**：确定与对照相比，您将在变体中更改的元素。
  4. **创建变体**：开发产品的替代版本，其中包括您想要测试的更改。
  5. **选择受众**：选择测试的目标受众，确保其代表您的用户群。
  6. **确定分配**：决定如何在对照组和变体组之间分配受众。
  7. **确保有效性**：检查您的测试是否存在可能影响结果的偏差和混杂变量。
  8. **运行测试**：向选定的受众部署 A/B 测试，监控每个组的表现。
  9. **收集数据**：收集有关每个组如何与相应版本的产品交互的数据。
  10. **分析结果**：使用统计方法确定对照和变体之间是否存在显着差异。
  11. **做出决定**：根据分析，决定是否实施更改、运行额外测试或放弃变体。
  12. **记录结果**：记录测试的结果和见解，以供将来参考和组织学习。
  13. **实施更改**：如果变体成功，请将更改推广到所有用户。

#### A/B 测试常用的工具有哪些？

[A/B testing](../A/a-b-testing.md) 的常用工具包括：

- **Optimizely**：一个用户友好的平台，提供广泛的 A/B 测试功能，允许跨网站和移动应用程序轻松进行实验。
  - **Google Optimize**：与 Google Analytics 集成，它是运行 A/B 测试的免费工具，对于中小型企业特别有用。
  - **VWO（可视化网站优化器）**：提供 A/B 测试以及其他测试功能，例如多变量测试和分割 URL 测试。
  - **Unbounce**：主要是一个登陆页面构建器，它还提供 A/B 测试功能来优化转化率。
  - **Adobe Target**：Adobe Marketing Cloud 的一部分，它是用于个性化和 A/B 测试的强大工具，适合企业级需求。
  - **Convert**：一种强调隐私和合规性的工具，提供 A/B 测试以及多变量和分割 URL 测试。
  - **Kameleoon**：一个全栈测试平台，为 Web 和移动应用程序提供 A/B 测试和个性化，重点关注人工智能驱动的见解。
  每个工具都有自己的一组功能和集成功能，因此选择通常取决于项目的具体需求，例如测试的复杂性、流量、与其他工具的集成以及所需的分析级别。

- **Optimizely**：一个用户友好的平台，提供广泛的 A/B 测试功能，允许跨网站和移动应用程序轻松进行实验。
  - **Google Optimize**：与 Google Analytics 集成，它是运行 A/B 测试的免费工具，对于中小型企业特别有用。
  - **VWO（可视化网站优化器）**：提供 A/B 测试以及其他测试功能，例如多变量测试和分割 URL 测试。
  - **Unbounce**：主要是一个登陆页面构建器，它还提供 A/B 测试功能来优化转化率。
  - **Adobe Target**：Adobe Marketing Cloud 的一部分，它是用于个性化和 A/B 测试的强大工具，适合企业级需求。
  - **Convert**：一种强调隐私和合规性的工具，提供 A/B 测试以及多变量和分割 URL 测试。
  - **Kameleoon**：一个全栈测试平台，为 Web 和移动应用程序提供 A/B 测试和个性化，重点关注人工智能驱动的见解。

#### 如何确定 A/B 测试的样本量？

确定 A/B 测试的**样本大小**对于确保测试有足够的功效来检测两个变体之间有意义的差异至关重要。这是一个简洁的指南：

1. **定义基线转化率 (BCR)**：使用历史数据建立对照组的 BCR。
  2. **建立最小可检测效应 (MDE)**：确定对您的业务实际意义重大的转化率最小变化。
  3. **选择显着性水平 (alpha)**：通常设置为 0.05，这是当原假设为真时拒绝原假设的概率（I 类错误）。
  4. **设置功效 (1 - beta)**：通常为 0.80，功效是当备择假设为真（1 - II 类错误）时正确拒绝原假设的概率。
  5. **计算样本量**：使用样本量计算器或统计软件。输入 BCR、MDE、alpha 和功效以获得每组所需的样本量。

  ```
  // Example using a hypothetical sample size function
  const sampleSize = calculateSampleSize({
    baselineConversionRate: 0.10,
    minimumDetectableEffect: 0.02,
    alpha: 0.05,
    power: 0.80
  });
  ```

1. **根据实际考虑进行调整**：考虑可用的流量和测试的持续时间。如果计算出的样本量太大，您可能需要增加 MDE 或降低功效以获得可行的样本量。
  请记住，样本量越大，结果就越精确，但获得这些结果所需的时间和成本也会更高。这是针对您的具体情况找到适当​​的平衡。

1. **定义基线转化率 (BCR)**：使用历史数据建立对照组的 BCR。
  2. **建立最小可检测效应 (MDE)**：确定对您的业务实际意义重大的转化率最小变化。
  3. **选择显着性水平 (alpha)**：通常设置为 0.05，这是当原假设为真时拒绝原假设的概率（I 类错误）。
  4. **设置功效 (1 - beta)**：通常为 0.80，功效是当备择假设为真（1 - II 类错误）时正确拒绝原假设的概率。
  5. **计算样本量**：使用样本量计算器或统计软件。输入 BCR、MDE、alpha 和功效以获得每组所需的样本量。
  1. **根据实际考虑进行调整**：考虑可用的流量和测试的持续时间。如果计算出的样本量太大，您可能需要增加 MDE 或降低功效以获得可行的样本量。

#### A/B 测试中的对照和变体是什么？

在[A/B testing](../A/a-b-testing.md) 中，**控件** 是正在测试的变量的原始版本，通常代表当前的用户体验或产品功能集。它作为新变体或**变体**进行比较的基准。该变体体现了正在测试的更改，例如号召性用语按钮的不同颜色或替代结账流程。
  控件有时被称为“A”版本，而变体则被称为“B”版本。进行 A/B 测试时，流量或用户在控制组和变体之间随机分配，确保每组在统计上相似。这种随机化有助于将变量变化的影响与其他外部因素隔离。
  然后根据预定义的指标（例如转化率或点击率）监控和衡量每个组的表现。通过比较这些指标，[test automation](../T/test-automation.md) 工程师可以确定变体是否比对照更有效地影响用户行为。如果变体在统计显着性上优于对照，则可以将其作为所有用户的新默认选项来实现。

### 分析与解读

#### A/B 测试的结果如何分析？

分析 A/B 测试的结果涉及比较对照组 (A) 和变异组 (B) 的性能指标，以确定行为或结果是否存在统计上的显着差异。主要步骤包括：

1. **数据收集**：在测试期间收集两组数据。
  2. **数据清理**：通过消除异常和异常值来确保数据质量。
  3. **计算绩效指标**：计算两个组的关键指标，例如转化率、点击率或任何其他相关 KPI。
  4. **统计分析**：
    - 执行一个
      **假设检验**
      （例如，t 检验、卡方检验）来比较组之间的指标。

- 计算
      **p值**
      评估观察到的差异偶然发生的概率。

- 确定 p 值是否低于预定义的值
      **显着性水平**
      （通常为0.05），表明有统计学上的显着差异。

- 执行一个
      **假设检验**
      （例如，t 检验、卡方检验）来比较组之间的指标。

- 计算
      **p值**
      评估观察到的差异偶然发生的概率。

- 确定 p 值是否低于预定义的值
      **显着性水平**
      （通常为0.05），表明有统计学上的显着差异。

5. **置信区间**：计算估计效果大小的置信区间，以了解真实效果在一定置信水平（通常为 95%）下所处的范围。
  如果变体在统计显着性上优于对照，则表明所做的改变产生了积极的影响。然而，也要考虑**实际意义**；即使结果具有统计显着性，也可能不足以保证实施。此外，检查测试是否存在可能影响结果有效性的潜在偏差或错误。经过彻底分析后，就是否将变体的更改实施到产品中做出数据驱动的决策。

1. **数据收集**：在测试期间收集两组数据。
  2. **数据清理**：通过消除异常和异常值来确保数据质量。
  3. **计算绩效指标**：计算两个组的关键指标，例如转化率、点击率或任何其他相关 KPI。
  4. **统计分析**：
    - 执行一个
      **假设检验**
      （例如，t 检验、卡方检验）来比较组之间的指标。

- 计算
      **p值**
      评估观察到的差异偶然发生的概率。

- 确定 p 值是否低于预定义的值
      **显着性水平**
      （通常为0.05），表明有统计学上的显着差异。

- 执行一个
      **假设检验**
      （例如，t 检验、卡方检验）来比较组之间的指标。

- 计算
      **p值**
      评估观察到的差异偶然发生的概率。

- 确定 p 值是否低于预定义的值
      **显着性水平**
      （通常为0.05），表明有统计学上的显着差异。

5. **置信区间**：计算估计效果大小的置信区间，以了解真实效果在一定置信水平（通常为 95%）下所处的范围。

#### A/B 测试中使用哪些统计方法？

统计方法是 **[A/B testing](../A/a-b-testing.md)** 不可或缺的一部分，提供了制定数据驱动决策的框架。主要统计方法包括：

- **假设检验**：确定对照和变体之间的性能差异是否具有统计显着性。通常涉及原假设（无差异）和备择假设（存在差异）。
  - **p 值计算**：在原假设成立的情况下，测量观察到结果的概率。较低的 p 值（通常低于 0.05）表明观察到的差异不太可能是偶然发生的，从而导致原假设被拒绝。
  - **置信区间**：提供一系列值，在该范围内，真实效果大小具有一定的置信度（通常为 95%）。如果置信区间不包括零，则结果被认为具有统计显着性。
  - **t检验**：在具有相似方差的正态分布数据的情况下，比较两组的均值。当方差不相等时，会使用韦尔奇 t 检验等变体。
  - **卡方检验**：评估分类数据以了解变量之间是否存在显着关联。
  - **贝叶斯方法**：提供传统频率统计的替代方法，提供给定数据的假设概率，而不是给定假设的数据概率。
  - **功效分析**：用于确定检测具有所需功效（通常为 0.8）和显着性水平的给定大小的效果所需的最小样本量。
  这些方法应用于从 A/B 测试中收集的数据，以得出有关变体与对照相比的影响的结论。正确的应用可确保获得可靠且可行的结果，指导产品开发中的明智决策。

- **假设检验**：确定对照和变体之间的性能差异是否具有统计显着性。通常涉及原假设（无差异）和备择假设（存在差异）。
  - **p 值计算**：在原假设成立的情况下，测量观察到结果的概率。较低的 p 值（通常低于 0.05）表明观察到的差异不太可能是偶然发生的，从而导致原假设被拒绝。
  - **置信区间**：提供一系列值，在该范围内，真实效果大小具有一定的置信度（通常为 95%）。如果置信区间不包括零，则结果被认为具有统计显着性。
  - **t检验**：在具有相似方差的正态分布数据的情况下，比较两组的均值。当方差不相等时，会使用韦尔奇 t 检验等变体。
  - **卡方检验**：评估分类数据以了解变量之间是否存在显着关联。
  - **贝叶斯方法**：提供传统频率统计的替代方法，提供给定数据的假设概率，而不是给定假设的数据概率。
  - **功效分析**：用于确定检测具有所需功效（通常为 0.8）和显着性水平的给定大小的效果所需的最小样本量。

#### 您如何解释 A/B 测试的结果？

解释 A/B 测试的结果涉及比较对照组 (A) 和变体组 (B) 的性能指标，以确定是否存在统计显着差异。测试结束后，您通常会获得一个数据集，其中包含每个组的关键指标，例如转化率、点击率或其他相关 KPI。
  首先，计算两组之间表现的**差异**。例如，如果您要衡量转化率，请从 B 组的转化率中减去 A 组的转化率。
  接下来，执行**统计显着性检验**，例如 t 检验或卡方检验，以确定观察到的差异是否是偶然造成的，或者是否可能是由于变体中所做的更改造成的。您将获得一个 p 值，将其与预先确定的显着性水平（通常为 0.05）进行比较。如果 p 值低于显着性水平，则结果被认为具有统计显着性。
  此外，还可以计算**置信区间**，以了解在一定置信水平（通常为 95%）下各组之间的真实差异所在的范围。
  最后，考虑结果的**实际意义**。即使结果具有统计显着性，也可能不足以保证对产品进行更改。在做出决定之前，请查看效果大小并考虑业务影响，包括潜在的投资回报率。
  请记住考虑可能影响结果的外部因素，并确保测试运行足够长的时间以捕获典型的用户行为。

#### A/B 测试中的统计显着性是什么？

[A/B testing](../A/a-b-testing.md) 背景下的统计显着性衡量的是我们对测试组（对照组和变体）之间观察到的差异是由于所做的更改而不是随机机会造成的信心程度。它使用 **p 值** 进行量化，它表示获得观察到的结果的概率，或更极端的是，如果组之间没有实际差异（零假设）。
  如果**p 值低于预定义阈值**（通常为 0.05），则结果通常被认为具有统计显着性。这意味着观察到的差异是由于随机变化造成的可能性不到 5%。 p 值越低，统计显着性越大。
  要确定统计显着性，您通常会使用统计检验，例如 **t 检验** 或 **卡方检验**，具体取决于您正在分析的数据类型。这些测试根据 A/B 测试的数据计算 p 值。
  统计显着性有助于就是否实施测试的变更做出明智的决定。然而，还必须考虑**实际意义**或变化对用户行为的实际影响，这可能并不总是仅通过统计显着性来反映。

#### 如何处理 A/B 测试中的误报或误报？

处理 [false positives](../F/false-positive.md) 或 [A/B testing](../A/a-b-testing.md) 中的负数涉及几个关键步骤：

- **验证测试[setup](../S/setup.md)** ：确保正确实施跟踪代码并且正确配置变体和控制组。
  - **检查外部因素**：识别可能影响测试结果的任何外部事件或变化，例如假期、停电或营销活动。
  - **审查细分**：确保正确定义受众群体，并且组之间没有重叠或污染。
  - **分析数据收集**：确认对照组和变异组的数据收集准确且一致。
  - **重新评估样本量**：确保样本量足够大，能够检测到有意义的差异，并且测试运行的时间足够长，能够达到统计显着性。
  - **使用测试后分析**：应用细分分析或群组分析等技术来更深入地挖掘结果并了解不同用户组的行为。
  - **运行后续测试**：如果结果不确定或怀疑假阳性或阴性，请进行后续测试以验证结果。
  通过系统地审查这些领域，您可以识别并纠正 [false positives](../F/false-positive.md) 或负面结果，确保您的 A/B 测试结果可靠且可操作。

- **验证测试[setup](../S/setup.md)**：确保正确实施跟踪代码并且正确配置变体和控制组。
  - **检查外部因素**：识别可能影响测试结果的任何外部事件或变化，例如假期、停电或营销活动。
  - **审查细分**：确保正确定义受众群体，并且组之间没有重叠或污染。
  - **分析数据收集**：确认对照组和变异组的数据收集准确且一致。
  - **重新评估样本量**：确保样本量足够大，能够检测到有意义的差异，并且测试运行的时间足够长，能够达到统计显着性。
  - **使用测试后分析**：应用细分分析或群组分析等技术来更深入地挖掘结果并了解不同用户组的行为。
  - **运行后续测试**：如果结果不确定或怀疑假阳性或阴性，请进行后续测试以验证结果。

### 高级概念

#### 什么是多变量测试以及它与 A/B 测试有何不同？

多变量测试 (MVT) 是一种用于同时测试多个变量以确定改善特定结果的最佳更改组合的技术。与比较单个变量的两个版本的 **[A/B testing](../A/a-b-testing.md)** 不同，MVT 可以涉及多个变量及其排列。
  在 MVT 中，您可以同时测试多个元素的变体，例如标题、图像和号召性用语按钮。这会创建一个可能组合的矩阵，每个组合都会呈现给一部分用户。主要优点是能够观察不同元素如何相互作用以及对用户行为的综合影响。
  由于变异数量增加，MVT 的复杂性需要更大的样本量才能达到统计显着性。在 [setup](../S/setup.md) 和分析方面，它也更加资源密集。然而，它可以提供关于变革如何协同作用的更全面的见解，从而有可能带来更优化的结果。
  相比之下，[A/B testing](../A/a-b-testing.md) 实施起来更简单、更快，一次专注于一项更改的影响。它通常用于针对单个更改或资源有限时做出决策。
  总而言之，[A/B testing](../A/a-b-testing.md) 比较单个更改的两个版本，而多变量测试评估多个更改及其交互的性能，需要更多资源，但可以更深入地了解修改的最佳组合。

#### 什么是分割 URL 测试？

分割 URL 测试是 [A/B testing](../A/a-b-testing.md) 的变体，其中流量在两个不同的 URL 之间分割，而不是在同一 URL 内同一页面的不同版本之间分割。在比较两个不同的页面设计、后端进程或托管在不同 URL 上的整个网站时，此方法特别有用。
  在拆分 URL 测试中，用户被随机定向到其中一个 URL，并跟踪他们与页面的交互，以确定哪个版本在预定义指标（例如转化率、页面停留时间或点击率）方面表现更好。
  **与传统 [A/B testing](../A/a-b-testing.md) 的主要区别**包括：

- **单独的 URL**：每个版本的测试都位于其自己的 URL 上。
  - **后端更改**：它允许测试可能涉及后端更改的重大更改。
  - **复杂的更改**：非常适合测试完全不同的布局或工作流程。
  要实施拆分 URL 测试，您通常会在服务器上使用重定向机制或测试工具，根据预定义的规则将传入流量定向到不同的 URL。确保流量分配是随机的并且其他因素（如用户位置、设备等）不会影响结果，这一点非常重要。
  分析结果涉及比较两个 URL 的性能指标，以确定哪一个更有效地实现预期目标。与[A/B testing](../A/a-b-testing.md) 一样，统计显着性对于确保结果不是偶然的至关重要。
  以下是如何在 `.htaccess` 文件中设置用于分割 URL 测试的重定向的基本示例：

  ```
  RewriteEngine On
  RewriteCond %{QUERY_STRING} ^version=a$
  RewriteRule ^page$ http://example.com/page-version-a [R=302,L]
  RewriteCond %{QUERY_STRING} ^version=b$
  RewriteRule ^page$ http://example.com/page-version-b [R=302,L]
  ```在此示例中，访问`http://example.com/page?version=a` 的用户将被重定向到与访问`http://example.com/page?version=b` 的用户不同的页面版本。

- **单独的 URL**：每个版本的测试都位于其自己的 URL 上。
  - **后端更改**：它允许测试可能涉及后端更改的重大更改。
  - **复杂的更改**：非常适合测试完全不同的布局或工作流程。

#### A/B 测试有哪些限制？

[A/B testing](../A/a-b-testing.md) 虽然功能强大，但有几个限制：

- **有限变量**：测试通常会比较两个版本，并更改​​单个变量。同时测试多个变量需要更复杂的多变量测试。
  - **耗时**：可能需要大量时间才能达到统计显着性，特别是对于低流量站点或微小更改。
  - **细分挑战**：结果可能无法解释不同用户细分的行为，如果样本不具有代表性，则可能会导致误导性结论。
  - **外部因素**：季节性、市场变化或其他外部因素可能会影响测试结果，因此很难将用户行为的变化单独归因于测试变量。
  - **交互影响**：用户体验的一部分的变化可能会影响另一部分，如果没有设计考虑此类交互，[A/B testing](../A/a-b-testing.md) 可能无法检测到。
  - **资源密集型**：需要资源来设计、实施、监控和分析，这可能会限制较小的团队或预算。
  - **道德考虑**：未经用户同意或使用敏感变量进行测试可能会引起道德问题。
  - **局部最大值**：[A/B testing](../A/a-b-testing.md) 非常适合优化，但可能会带来增量改进，可能会错过可能带来显着更好结果的创新想法。
  - **实现错误**：不正确的[setup](../S/setup.md)可能会导致错误的结果。正确的技术实施至关重要。
  - **数据解释**：可能会发生对数据的误解，尤其是在缺乏统计分析方面的专业知识的情况下。
  了解这些限制对于[test automation](../T/test-automation.md) 工程师确保有效使用[A/B testing](../A/a-b-testing.md) 并正确解释其结果至关重要。

- **有限变量**：测试通常会比较两个版本，并更改​​单个变量。同时测试多个变量需要更复杂的多变量测试。
  - **耗时**：可能需要大量时间才能达到统计显着性，特别是对于低流量站点或微小更改。
  - **细分挑战**：结果可能无法解释不同用户细分的行为，如果样本不具有代表性，则可能会导致误导性结论。
  - **外部因素**：季节性、市场变化或其他外部因素可能会影响测试结果，因此很难将用户行为的变化单独归因于测试变量。
  - **交互影响**：用户体验的一部分的变化可能会影响另一部分，如果没有设计考虑此类交互，[A/B testing](../A/a-b-testing.md) 可能无法检测到。
  - **资源密集型**：需要资源来设计、实施、监控和分析，这可能会限制较小的团队或预算。
  - **道德考虑**：未经用户同意或使用敏感变量进行测试可能会引起道德问题。
  - **局部最大值**：[A/B testing](../A/a-b-testing.md) 非常适合优化，但可能会带来增量改进，可能会错过可能带来显着更好结果的创新想法。
  - **实现错误**：不正确的[setup](../S/setup.md)可能会导致错误的结果。正确的技术实施至关重要。
  - **数据解释**：可能会发生对数据的误解，尤其是在缺乏统计分析方面的专业知识的情况下。

#### A/B 测试如何与其他测试方法结合使用？

[A/B testing](../A/a-b-testing.md)可以与各种测试方法集成，以增强[software quality](../S/software-quality.md)和用户体验。例如，**[unit testing](../U/unit-testing.md)** 确保在 A/B 测试比较不同用户流程之前各个组件正常运行。 **[Integration testing](../I/integration-testing.md)** 检查组合部件是否协同工作，这在 A/B 测试检查更改对集成系统的影响之前至关重要。
  将**自动化[regression testing](../R/regression-testing.md)** 与[A/B testing](../A/a-b-testing.md) 结合起来有利于确保新功能或更改不会破坏现有功能。自动化测试可以在向用户公开之前快速验证控制版本和变体版本是否稳定并按预期运行。
  **[Usability testing](../U/usability-testing.md)** 可以与 [A/B testing](../A/a-b-testing.md) 一起使用，以获得对用户行为和偏好的定性洞察。 [A/B testing](../A/a-b-testing.md) 量化了更改的影响，[usability testing](../U/usability-testing.md) 可以解释为什么某些更改效果更好。
  **[Performance testing](../P/performance-testing.md)** 应在[A/B testing](../A/a-b-testing.md) 之前进行，以确保两种变体都能提供可接受的响应时间并能够处理预期负载。这一点至关重要，因为性能会显着影响用户行为，进而影响 A/B 测试的结果。
  最后，在 [A/B testing](../A/a-b-testing.md) 期间应使用**监控和日志记录工具**来跟踪错误、性能指标和用户交互。这些数据对于解释 A/B 测试结果和诊断可能与正在测试的更改不直接相关的问题非常宝贵。
  通过将 [A/B testing](../A/a-b-testing.md) 与这些方法相结合，您可以确保对软件变更进行全面评估，从而做出更明智的决策和更高质量的产品。

#### A/B 测试中“均值回归”的概念是什么？

在[A/B testing](../A/a-b-testing.md) 的上下文中，**回归平均值**是指在后续测量中极端结果往往变得不那么极端的现象。当变体（A 或 B）在初始测试期间显示出与对照的显着差异，但这种差异在后续测试中减小或消失时，就会发生这种情况。
  在分析 A/B 测试的结果时，这种效应尤其重要。如果初始测试显示新功能或设计（变体）具有强大的性能，那么可能很容易将这种成功归因于所做的更改。然而，如果初始结果受到不一致的变量（例如临时用户行为、季节性影响或其他外部因素）的影响，则后续测试可能会表明性能优势不是由于变体本身，而是由于这些外部影响。
  为了降低由于均值回归而误解结果的风险，至关重要的是：

- **运行测试足够长的时间**
    平均掉异常情况。

- **重复测试**
    当结果异常高或低时以确认发现。

- **使用足够大的样本量**
    以尽量减少异常值的影响。

- **控制外部变量**
    尽可能确保测试条件一致。
  通过了解均值回归，[test automation](../T/test-automation.md) 工程师可以避免根据初始 A/B 测试结果对变更的效果做出过早的结论。

- **运行测试足够长的时间**
    平均掉异常情况。

- **重复测试**
    当结果异常高或低时以确认发现。

- **使用足够大的样本量**
    以尽量减少异常值的影响。

- **控制外部变量**
    尽可能确保测试条件一致。
